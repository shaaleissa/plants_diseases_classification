{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import glob\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization,InputLayer\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "print ('Modules loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input images and assigning labels based on folder names\n",
    "print(os.listdir(\"data/Plants_2\"))\n",
    "\n",
    "SIZE = (200,300)  #Resize images\n",
    "\n",
    "#Capturing training data and labels into respective lists\n",
    "train_images = []\n",
    "train_labels = [] \n",
    "dir='data/resized/train'\n",
    "\n",
    "\n",
    "for label in os.listdir(dir):\n",
    "    label_path = os.path.join(dir, label)\n",
    "    print(label)\n",
    "    for img_path in glob.glob(os.path.join(label_path, \"*.JPG\")):\n",
    "        print('inside')    \n",
    "        print(img_path)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
    "        img = cv2.resize(img, (SIZE))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        train_images.append(img_path)\n",
    "        train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.DataFrame({'image':train_images,'label':train_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input images and assigning labels based on folder names\n",
    "print(os.listdir(\"data/Plants_2\"))\n",
    "\n",
    "SIZE = (200,300)  #Resize images\n",
    "\n",
    "#Capturing training data and labels into respective lists\n",
    "valid_images = []\n",
    "valid_labels = [] \n",
    "dir = 'data/resized/valid'\n",
    "\n",
    "\n",
    "for label in os.listdir(dir):\n",
    "    label_path = os.path.join(dir, label)\n",
    "    print(label)\n",
    "    for img_path in glob.glob(os.path.join(label_path, \"*.JPG\")):\n",
    "        print('inside')    \n",
    "        print(img_path)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
    "        img = cv2.resize(img, (SIZE))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        valid_images.append(img_path)\n",
    "        valid_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df=pd.DataFrame({'image':valid_images,'label':valid_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input images and assigning labels based on folder names\n",
    "print(os.listdir(\"data/Plants_2\"))\n",
    "\n",
    "SIZE = (200,300) #Resize images\n",
    "\n",
    "#Capturing training data and labels into respective lists\n",
    "test_images = []\n",
    "test_labels = [] \n",
    "dir = 'data/resized/test'\n",
    "\n",
    "\n",
    "for label in os.listdir(dir):\n",
    "    label_path = os.path.join(dir, label)\n",
    "    print(label)\n",
    "    for img_path in glob.glob(os.path.join(label_path, \"*.JPG\")):\n",
    "        print('inside')    \n",
    "        print(img_path)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
    "        img = cv2.resize(img, (SIZE))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        test_images.append(img_path)\n",
    "        test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.DataFrame({'image':test_images,'label':test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(df, max_samples, min_samples, column):\n",
    "    df=df.copy()\n",
    "    classes=df[column].unique()\n",
    "    class_count=len(classes)\n",
    "    length=len(df)\n",
    "    print ('dataframe initially is of length ',length, ' with ', class_count, ' classes')\n",
    "    groups=df.groupby(column)    \n",
    "    trimmed_df = pd.DataFrame(columns = df.columns)\n",
    "    groups=df.groupby(column)\n",
    "    for label in df[column].unique(): \n",
    "        group=groups.get_group(label)\n",
    "        count=len(group)    \n",
    "        if count > max_samples:\n",
    "            sampled_group=group.sample(n=max_samples, random_state=123,axis=0)\n",
    "            trimmed_df=pd.concat([trimmed_df, sampled_group], axis=0)\n",
    "        else:\n",
    "            if count>=min_samples:\n",
    "                sampled_group=group        \n",
    "                trimmed_df=pd.concat([trimmed_df, sampled_group], axis=0)\n",
    "    print('after trimming, the maximum samples in any class is now ',max_samples, ' and the minimum samples in any class is ', min_samples)\n",
    "    classes=trimmed_df[column].unique()# return this in case some classes have less than min_samples\n",
    "    class_count=len(classes) # return this in case some classes have less than min_samples\n",
    "    length=len(trimmed_df)\n",
    "    print ('the trimmed dataframe now is of length ',length, ' with ', class_count, ' classes')\n",
    "    return trimmed_df, classes, class_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples=200\n",
    "min_samples=54\n",
    "column='label'\n",
    "train_df, classes, class_count=trim(train_df, max_samples, min_samples, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(['label']).size().plot.bar(figsize=(15, 5), title='Number of images per class before augmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(df, n, working_dir, img_size):\n",
    "    df=df.copy()\n",
    "    print('Initial length of dataframe is ', len(df))\n",
    "    aug_dir=os.path.join(working_dir, 'aug')# directory to store augmented images\n",
    "    if os.path.isdir(aug_dir):# start with an empty directory\n",
    "        shutil.rmtree(aug_dir)\n",
    "    os.mkdir(aug_dir)        \n",
    "    for label in df['label'].unique():    \n",
    "        dir_path=os.path.join(aug_dir,label)    \n",
    "        os.mkdir(dir_path) # make class directories within aug directory\n",
    "    # create and store the augmented images  \n",
    "    total=0\n",
    "    gen=ImageDataGenerator(horizontal_flip=True,  rotation_range=20, width_shift_range=.2,\n",
    "                                  height_shift_range=.2, zoom_range=.2)\n",
    "    groups=df.groupby('label') # group by class\n",
    "    for label in df['label'].unique():  # for every class               \n",
    "        group=groups.get_group(label)  # a dataframe holding only rows with the specified label \n",
    "        sample_count=len(group)   # determine how many samples there are in this class  \n",
    "        if sample_count< n: # if the class has less than target number of images\n",
    "            aug_img_count=0\n",
    "            delta=n - sample_count  # number of augmented images to create\n",
    "            target_dir=os.path.join(aug_dir, label)  # define where to write the images\n",
    "            msg='{0:40s} for class {1:^30s} creating {2:^5s} augmented images'.format(' ', label, str(delta))\n",
    "            print(msg, '\\r', end='') # prints over on the same line\n",
    "            aug_gen=gen.flow_from_dataframe( group,  x_col='image', y_col=None, target_size=img_size,\n",
    "                                            class_mode=None, batch_size=1, shuffle=False, \n",
    "                                            save_to_dir=target_dir, save_prefix='aug-', color_mode='rgb',\n",
    "                                            save_format='jpg')\n",
    "            while aug_img_count<delta:\n",
    "                images=next(aug_gen)            \n",
    "                aug_img_count += len(images)\n",
    "            total +=aug_img_count\n",
    "    print('Total Augmented images created= ', total)\n",
    "    # create aug_df and merge with train_df to create composite training set ndf\n",
    "    aug_fpaths=[]\n",
    "    aug_labels=[]\n",
    "    classlist=os.listdir(aug_dir)\n",
    "    for klass in classlist:\n",
    "        classpath=os.path.join(aug_dir, klass)     \n",
    "        flist=os.listdir(classpath)    \n",
    "        for f in flist:        \n",
    "            fpath=os.path.join(classpath,f)         \n",
    "            aug_fpaths.append(fpath)\n",
    "            aug_labels.append(klass)\n",
    "    Fseries=pd.Series(aug_fpaths, name='image')\n",
    "    Lseries=pd.Series(aug_labels, name='label')\n",
    "    aug_df=pd.concat([Fseries, Lseries], axis=1)         \n",
    "    df=pd.concat([df,aug_df], axis=0).reset_index(drop=True)\n",
    "    print('Length of augmented dataframe is now ', len(df))\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=200 # number of samples in each class\n",
    "working_dir=r'./data' # directory to store augmented images\n",
    "img_size=(200,300) # size of augmented images\n",
    "train_df=balance(train_df, n, working_dir, img_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(['label']).size().plot.bar(figsize=(15, 5), title='Number of images per class after augmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gens(batch_size, train_df, test_df, valid_df, img_size):\n",
    "    trgen=ImageDataGenerator()\n",
    "    t_and_v_gen=ImageDataGenerator()\n",
    "    msg='{0:70s} for train generator'.format(' ')\n",
    "    print(msg, '\\r', end='') # prints over on the same line\n",
    "    train_gen=trgen.flow_from_dataframe(train_df, x_col='image', y_col='label', target_size=img_size,\n",
    "                                       class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
    "    msg='{0:70s} for valid generator'.format(' ')\n",
    "    print(msg, '\\r', end='') # prints over on the same line\n",
    "    valid_gen=t_and_v_gen.flow_from_dataframe(valid_df, x_col='image', y_col='label', target_size=img_size,\n",
    "                                       class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)\n",
    "    # for the test_gen we want to calculate the batch size and test steps such that batch_size X test_steps= number of samples in test set\n",
    "    # this insures that we go through all the sample in the test set exactly once.\n",
    "    length=len(test_df)\n",
    "    test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \n",
    "    test_steps=int(length/test_batch_size)\n",
    "    msg='{0:70s} for test generator'.format(' ')\n",
    "    print(msg, '\\r', end='') # prints over on the same line\n",
    "    test_gen=t_and_v_gen.flow_from_dataframe(test_df, x_col='image', y_col='label', target_size=img_size,\n",
    "                                       class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
    "    # from the generator we can get information we will need later\n",
    "    classes=list(train_gen.class_indices.keys())\n",
    "    class_indices=list(train_gen.class_indices.values())\n",
    "    class_count=len(classes)\n",
    "    labels=test_gen.labels\n",
    "    print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps, ' number of classes : ', class_count)\n",
    "    return train_gen, test_gen, valid_gen, test_batch_size, test_steps, classes\n",
    "\n",
    "\n",
    "batch_size=20\n",
    "train_gen, test_gen, valid_gen, test_batch_size, test_steps, classes=make_gens(batch_size, train_df, test_df, valid_df, img_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_samples(gen ):\n",
    "    t_dict=gen.class_indices\n",
    "    classes=list(t_dict.keys())    \n",
    "    images,labels=next(gen) # get a sample batch from the generator \n",
    "    plt.figure(figsize=(25, 25))\n",
    "    length=len(labels)\n",
    "    if length<25:   #show maximum of 25 images\n",
    "        r=length\n",
    "    else:\n",
    "        r=25\n",
    "    for i in range(r):        \n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image=images[i] /255       \n",
    "        plt.imshow(image)\n",
    "        index=np.argmax(labels[i])\n",
    "        class_name=classes[index]\n",
    "        plt.title(class_name, color='blue', fontsize=18)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "show_image_samples(train_gen )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_in_color(txt_msg,fore_tupple=(0,255,255),back_tupple=(100,100,100)):\n",
    "    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n",
    "    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n",
    "    # default parameter print in cyan foreground and gray background\n",
    "    rf,gf,bf=fore_tupple\n",
    "    rb,gb,bb=back_tupple\n",
    "    msg='{0}' + txt_msg\n",
    "    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n",
    "    print(msg .format(mat), flush=True)\n",
    "    print('\\33[0m', flush=True) # returns default print color to back to black\n",
    "    return\n",
    "\n",
    "# example default print\n",
    "msg='test of default colors'\n",
    "print_in_color(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_ASK(keras.callbacks.Callback):\n",
    "    def __init__ (self, model, epochs,  ask_epoch, dwell=True, factor=.4): # initialization of the callback\n",
    "        super(LR_ASK, self).__init__()\n",
    "        self.model=model               \n",
    "        self.ask_epoch=ask_epoch\n",
    "        self.epochs=epochs\n",
    "        self.ask=True # if True query the user on a specified epoch\n",
    "        self.lowest_vloss=np.inf\n",
    "        self.lowest_aloss=np.inf\n",
    "        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
    "        self.best_epoch=1\n",
    "        self.plist=[]\n",
    "        self.alist=[]\n",
    "        self.dwell= dwell\n",
    "        self.factor=factor\n",
    "        \n",
    "    def get_list(self): # define a function to return the list of % validation change\n",
    "        return self.plist, self.alist\n",
    "    def on_train_begin(self, logs=None): # this runs on the beginning of training\n",
    "        if self.ask_epoch == 0: \n",
    "            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n",
    "            self.ask_epoch=1\n",
    "        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n",
    "            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n",
    "            self.ask=False # do not query the user\n",
    "        if self.epochs == 1:\n",
    "            self.ask=False # running only for 1 epoch so do not query user\n",
    "        else:\n",
    "            msg =f'Training will proceed until epoch {ask_epoch} then you will be asked to' \n",
    "            print_in_color(msg )\n",
    "            msg='enter H to halt training or enter an integer for how many more epochs to run then be asked again'\n",
    "            print_in_color(msg)\n",
    "            if self.dwell:\n",
    "                msg='learning rate will be automatically adjusted during training'\n",
    "                print_in_color(msg, (0,255,0))\n",
    "        self.start_time= time.time() # set the time at which training started\n",
    "       \n",
    "    def on_train_end(self, logs=None):   # runs at the end of training  \n",
    "        msg=f'loading model with weights from epoch {self.best_epoch}'\n",
    "        print_in_color(msg, (0,255,255))\n",
    "        model.set_weights(self.best_weights) # set the weights of the model to the best weights\n",
    "        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print_in_color (msg) # print out training duration time\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
    "        vloss=logs.get('val_loss')  # get the validation loss for this epoch\n",
    "        aloss=logs.get('loss')\n",
    "        if epoch >0:\n",
    "            deltav = self.lowest_vloss- vloss \n",
    "            pimprov=(deltav/self.lowest_vloss) * 100 \n",
    "            self.plist.append(pimprov)\n",
    "            deltaa=self.lowest_aloss-aloss\n",
    "            aimprov=(deltaa/self.lowest_aloss) * 100\n",
    "            self.alist.append(aimprov)\n",
    "        else:\n",
    "            pimprov=0.0 \n",
    "            aimprov=0.0\n",
    "        if vloss< self.lowest_vloss:\n",
    "            self.lowest_vloss=vloss\n",
    "            self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
    "            self.best_epoch=epoch + 1            \n",
    "            msg=f'\\n validation loss of {vloss:7.4f} is {pimprov:7.4f} % below lowest loss, saving weights from epoch {str(epoch + 1):3s} as best weights'\n",
    "            print_in_color(msg, (0,255,0)) # green foreground\n",
    "        else: # validation loss increased\n",
    "            pimprov=abs(pimprov)\n",
    "            msg=f'\\n validation loss of {vloss:7.4f} is {pimprov:7.4f} % above lowest loss of {self.lowest_vloss:7.4f} keeping weights from epoch {str(self.best_epoch)} as best weights'\n",
    "            print_in_color(msg, (255,255,0)) # yellow foreground\n",
    "            if self.dwell: # if dwell is True when the validation loss increases the learning rate is automatically reduced and model weights are set to best weights\n",
    "                lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "                new_lr=lr * self.factor\n",
    "                msg=f'learning rate was automatically adjusted from {lr:8.6f} to {new_lr:8.6f}, model weights set to best weights'\n",
    "                print_in_color(msg) # cyan foreground\n",
    "                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n",
    "                model.set_weights(self.best_weights) # set the weights of the model to the best weights      \n",
    "                \n",
    "        if aloss< self.lowest_aloss:\n",
    "            self.lowest_aloss=aloss        \n",
    "        if self.ask: # are the conditions right to query the user?\n",
    "            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n",
    "                msg='\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again'\n",
    "                print_in_color(msg) # cyan foreground\n",
    "                ans=input()\n",
    "                \n",
    "                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n",
    "                    msg=f'you entered {ans},  Training halted on epoch {epoch+1} due to user input\\n'\n",
    "                    print_in_color(msg)\n",
    "                    self.model.stop_training = True # halt training\n",
    "                else: # user wants to continue training\n",
    "                    self.ask_epoch += int(ans)\n",
    "                    if self.ask_epoch > self.epochs:\n",
    "                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n",
    "                    else:\n",
    "                        msg=f'you entered {ans} Training will continue to epoch {self.ask_epoch}'\n",
    "                        print_in_color(msg) # cyan foreground\n",
    "                        if self.dwell==False:\n",
    "                            lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "                            msg=f'current LR is  {lr:8.6f}  hit enter to keep  this LR or enter a new LR'\n",
    "                            print_in_color(msg) # cyan foreground\n",
    "                            ans=input(' ')\n",
    "                            if ans =='':\n",
    "                                msg=f'keeping current LR of {lr:7.5f}'\n",
    "                                print_in_color(msg) # cyan foreground\n",
    "                            else:\n",
    "                                new_lr=float(ans)\n",
    "                                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n",
    "                                msg=f' changing LR to {ans}'\n",
    "                                print_in_color(msg) # cyan foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(200, 300, 3)))\n",
    "\n",
    "# 1st conv block\n",
    "model.add(Conv2D(25, (5, 5), activation='relu', strides=(1, 1), padding='same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "# 2nd conv block\n",
    "model.add(Conv2D(50, (5, 5), activation='relu', strides=(2, 2), padding='same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "# 3rd conv block\n",
    "model.add(Conv2D(70, (3, 3), activation='relu', strides=(2, 2), padding='same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "# ANN block\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "# output layer\n",
    "model.add(Dense(units=22, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=.001\n",
    "model.compile(optimizer=Adam(learning_rate=lr), \n",
    "              \n",
    "              loss='categorical_crossentropy',\n",
    "              \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=40\n",
    "ask_epoch=10\n",
    "ask=LR_ASK(model, epochs,  ask_epoch)\n",
    "callbacks=[ask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_gen, epochs=40, verbose=1, callbacks=callbacks,\n",
    "                    validation_data=valid_gen,\n",
    "                    validation_steps=None,  shuffle=False,  initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_plot(tr_data, start_epoch):\n",
    "    #Plot the training and validation data\n",
    "    tacc=tr_data.history['accuracy']\n",
    "    tloss=tr_data.history['loss']\n",
    "    vacc=tr_data.history['val_accuracy']\n",
    "    vloss=tr_data.history['val_loss']\n",
    "    Epoch_count=len(tacc)+ start_epoch\n",
    "    Epochs=[]\n",
    "    for i in range (start_epoch ,Epoch_count):\n",
    "        Epochs.append(i+1)   \n",
    "    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n",
    "    val_lowest=vloss[index_loss]\n",
    "    index_acc=np.argmax(vacc)\n",
    "    acc_highest=vacc[index_acc]\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
    "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
    "    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(25,10))\n",
    "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
    "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
    "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
    "    axes[0].scatter(Epochs, tloss, s=100, c='red')    \n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs', fontsize=18)\n",
    "    axes[0].set_ylabel('Loss', fontsize=18)\n",
    "    axes[0].legend()\n",
    "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
    "    axes[1].scatter(Epochs, tacc, s=100, c='red')\n",
    "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
    "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epochs', fontsize=18)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=18)\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout    \n",
    "    plt.show()\n",
    "    return index_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_index=tr_plot(history,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the weights of the model to the best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(test_gen):    \n",
    "    y_pred= []\n",
    "    error_list=[]\n",
    "    error_pred_list = []\n",
    "    y_true=test_gen.labels\n",
    "    classes=list(test_gen.class_indices.keys())\n",
    "    class_count=len(classes)\n",
    "    errors=0\n",
    "    preds=model.predict(test_gen, verbose=1)\n",
    "    tests=len(preds)    \n",
    "    for i, p in enumerate(preds):  \n",
    "        file=test_gen.filenames[i]        \n",
    "        pred_index=np.argmax(p)         \n",
    "        true_index=test_gen.labels[i]  # labels are integer values        \n",
    "        if pred_index != true_index: # a misclassification has occurred                                           \n",
    "            errors=errors + 1\n",
    "            file=test_gen.filenames[i]            \n",
    "            error_class=classes[pred_index]\n",
    "            t=(file, error_class)\n",
    "            error_list.append(t)            \n",
    "        y_pred.append(pred_index)\n",
    "            \n",
    "    acc=( 1-errors/tests) * 100\n",
    "    msg=f'there were {errors} errors in {tests} tests for an accuracy of {acc:6.2f}'\n",
    "    print_in_color(msg, (0,255,255), (100,100,100)) # cyan foreground\n",
    "    ypred=np.array(y_pred)\n",
    "    ytrue=np.array(y_true)\n",
    "    f1score=f1_score(ytrue, ypred, average='weighted')* 100\n",
    "    if class_count <=30:\n",
    "        cm = confusion_matrix(ytrue, ypred )\n",
    "        # plot the confusion matrix\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "        plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n",
    "        plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "    clr = classification_report(y_true, y_pred, target_names=classes, digits= 4) # create classification report\n",
    "    print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "    return errors, tests, error_list, f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors, tests, error_list, f1score =predictor(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_predict_path='data/images to predict'\n",
    "image_to_predict=os.listdir(images_to_predict_path)\n",
    "\n",
    "images=[]\n",
    "for image in range(len(image_to_predict)):\n",
    "    images.append(os.path.join(images_to_predict_path+'/'+image_to_predict[image]))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    img = tf.keras.utils.load_img(\n",
    "        images[i], target_size=(200, 300)\n",
    "    )\n",
    "\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"This image from ( {} ) most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(images[i],classes[np.argmax(score)], 100 * np.max(score))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject='plants' \n",
    "save_id=f'{subject}-{f1score:5.2f}.h5'\n",
    "model_save_loc=os.path.join(working_dir, save_id)\n",
    "model.save(model_save_loc)\n",
    "msg= f'model was saved as {model_save_loc}'\n",
    "print_in_color(msg, (0,255,255), (100,100,100)) # cyan foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_df=pd.concat([train_df,test_df], ignore_index=True)\n",
    "df=pd.concat([half_df,valid_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['label']).size().plot.bar(figsize=(15, 5), title='Number of images per class')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roc Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model('plants-81.23.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_y=next(train_gen)\n",
    "test_X,test_y=next(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = loaded_model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy=loaded_model.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
